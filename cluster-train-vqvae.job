#!/bin/bash

# Train vqvae
# USAGE:
# sbatch cluster-train-vqvae.job
#
#SBATCH --job-name=vqvae
#SBATCH --mail-user=echandler@uchicago.edu
#SBATCH --mail-type=ALL
#
#SBATCH --output=/home/echandler/DeepL/final-project/mi-vqvae/output/%j.%N.stdout
#SBATCH --error=/home/echandler/DeepL/final-project/mi-vqvae/output/%j.%N.stderr
#
#OPTIONS FOR JOB SIZE:
#SBATCH --partition=general
#SBATCH --nodes=1
# NOTE: always set ntasks==gpus or else it doesnt allocate right?
#SBATCH --ntasks=1
#SBATCH --gpus=1
#SBATCH --gpus-per-task=1
#NOTSBATCH --gres=gpu:rtx8000:1
#SBATCH --mem-per-cpu=32G
#SBATCH --time=01:00:00
#

PROJECT_ROOT="/home/echandler/DeepL/final-project"
VQVAE_DIR="$PROJECT_ROOT/mi-vqvae"
VQVAE_REPO="$VQVAE_DIR/pytorch-vqvae"
CELEBA_DIR="/net/scratch/echandler/datasets/img_align_celeba"
#CELEBA_DIR="/net/scratch/echandler/datasets/celeba"
MNIST_DIR="$PROJECT_ROOT/data/mnist"

#. /home/echandler/.bashrc
#conda activate vqvae
echo "Env sanity check: py, hostname, cuda devices."
which python
hostname
echo $CUDA_VISIBLE_DEVICES

cd "$VQVAE_REPO"
python vqvae.py --dataset celeba \
    --data-folder "$CELEBA_DIR" \
    --run-name celeba-b256-f32 \
    --model encoder \
    --batch-size 256 \
    --num-epochs 50 \
    --num-workers 2 \
    --device cuda \
    --logger-lvl INFO
